{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAG3gowRZtFA"
      },
      "source": [
        "# Vector space with ML\n",
        "\n",
        "This lab will be devoted to the use of ML model for the needs of information retrieval and text classification.  \n",
        "\n",
        "**Searching in the curious facts database**\n",
        "\n",
        "The facts dataset is given [here](https://raw.githubusercontent.com/IUCVLab/information-retrieval/main/datasets/facts.txt), take a look. We want you to retrieve facts **relevant to the query** (whatever it means), for example, you type \"good mood\", and get to know that Cherophobia is the fear of fun. For this, the idea is to utilize document vectors. However, instead of forming vectors with tf-idf and reducing dimensions, this time we want to obtain fixed-size vectors for documents using ML model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8NV0YmcZtFE"
      },
      "source": [
        "## 1. Use neural networks to embed sentences\n",
        "\n",
        "Make use of any, starting from doc2vec up to Transformers, etc. Provide all code, dependencies, installation requirements.\n",
        "\n",
        "\n",
        "- [UCE in spacy 2](https://spacy.io/universe/project/spacy-universal-sentence-encoder) (`!pip install spacy-universal-sentence-encoder`)\n",
        "- [Sentence BERT in spacy 2](https://spacy.io/universe/project/spacy-sentence-bert) (`!pip install spacy-sentence-bert`)\n",
        "- [Pretrained ðŸ¤— Transformers](https://huggingface.co/transformers/pretrained_models.html)\n",
        "- [Spacy 3 transformers](https://spacy.io/usage/embeddings-transformers#transformers-installation)\n",
        "- [doc2vec pretrained](https://github.com/jhlau/doc2vec)\n",
        "- [Some more sentence transformers](https://www.sbert.net/docs/quickstart.html)\n",
        "- [Even fasttext can do a sentence embedding](https://fasttext.cc/docs/en/python-module.html#model-object)\n",
        "\n",
        "Here should be dependency installation, download instructions and so on. With outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUzC7X8mZtFF",
        "outputId": "55e2505e-1882-42e1-f9d5-d951238d3db9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy-sentence-bert in /usr/local/lib/python3.9/dist-packages (0.1.2)\n",
            "Collecting spacy-universal-sentence-encoder\n",
            "  Downloading spacy_universal_sentence_encoder-0.4.5.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: spacy<4.0.0,>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy-sentence-bert) (3.4.4)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.9/dist-packages (from spacy-sentence-bert) (2.2.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.9/dist-packages (from spacy-sentence-bert) (3.19.6)\n",
            "Requirement already satisfied: tensorflow<3.0.0,>=2.4.0 in /usr/local/lib/python3.9/dist-packages (from spacy-universal-sentence-encoder) (2.11.0)\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.9/dist-packages (from spacy-universal-sentence-encoder) (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-sentence-bert) (1.22.4)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-sentence-bert) (0.7.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-sentence-bert) (3.3.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-sentence-bert) (8.1.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-sentence-bert) (2.0.7)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-sentence-bert) (4.65.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-sentence-bert) (3.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-sentence-bert) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-sentence-bert) (1.0.9)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-sentence-bert) (3.0.12)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-sentence-bert) (2.4.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-sentence-bert) (3.1.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-sentence-bert) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-sentence-bert) (23.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-sentence-bert) (6.3.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-sentence-bert) (0.10.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-sentence-bert) (1.10.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-sentence-bert) (0.10.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-sentence-bert) (1.0.4)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-sentence-bert) (2.0.8)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (0.31.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (2.11.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (3.3.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (15.0.6.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (1.51.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (2.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (23.3.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (1.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (1.15.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (0.4.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (1.15.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (2.11.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (4.5.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (2.11.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (0.2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (1.6.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from sentence-transformers->spacy-sentence-bert) (1.2.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence-transformers->spacy-sentence-bert) (3.7)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers->spacy-sentence-bert) (1.10.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence-transformers->spacy-sentence-bert) (0.14.1+cu116)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (from sentence-transformers->spacy-sentence-bert) (0.1.97)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers->spacy-sentence-bert) (4.26.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers->spacy-sentence-bert) (1.13.1+cu116)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers->spacy-sentence-bert) (0.13.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (0.38.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers->spacy-sentence-bert) (3.9.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers->spacy-sentence-bert) (6.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-sentence-bert) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-sentence-bert) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-sentence-bert) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-sentence-bert) (4.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (2.16.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (2.2.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (1.8.1)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4.0.0,>=3.0.0->spacy-sentence-bert) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4.0.0,>=3.0.0->spacy-sentence-bert) (0.7.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers->spacy-sentence-bert) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers->spacy-sentence-bert) (0.13.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy-sentence-bert) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy<4.0.0,>=3.0.0->spacy-sentence-bert) (2.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers->spacy-sentence-bert) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->sentence-transformers->spacy-sentence-bert) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence-transformers->spacy-sentence-bert) (8.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (6.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder) (3.2.2)\n",
            "Building wheels for collected packages: spacy-universal-sentence-encoder\n",
            "  Building wheel for spacy-universal-sentence-encoder (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spacy-universal-sentence-encoder: filename=spacy_universal_sentence_encoder-0.4.5-py3-none-any.whl size=15810 sha256=41f9f6962128bb38935965cca92bd70e7b8b1c4a841f32d3d85cc37a4a503b81\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/49/92/b6625f8403154da77a7def3772041f59694dbd972c6b5f9ecf\n",
            "Successfully built spacy-universal-sentence-encoder\n",
            "Installing collected packages: spacy-universal-sentence-encoder\n",
            "Successfully installed spacy-universal-sentence-encoder-0.4.5\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy-sentence-bert spacy-universal-sentence-encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-MxRnQNZtFH"
      },
      "source": [
        "And then use the library to download (and load) the model.\n",
        "\n",
        "NB: model downloading may take time (depending on the model hosting). If you think it may take a long time, ask your TA for assistance with binaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "scrolled": true,
        "id": "ttLACoKIZtFI"
      },
      "outputs": [],
      "source": [
        "import tensorflow_hub as hub\n",
        "embed_hub = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = embed_hub([\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"I am a sentence for which I would like to get its embedding\"])\n",
        "\n",
        "print(embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpmjrQdHilQa",
        "outputId": "80bd0e0d-5b96-4181-8096-173cc15b9071"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[ 0.01305104  0.02235124 -0.03263276 ... -0.00565091 -0.04793027\n",
            "  -0.11492757]\n",
            " [ 0.05833391 -0.08185011  0.06890937 ... -0.00923876 -0.08695353\n",
            "  -0.01415738]], shape=(2, 512), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZuQWTd5ZtFJ"
      },
      "source": [
        "## 2. Write a function that prepares embedding of arbitrary queries\n",
        "\n",
        "Write a function, which returns a fixed-sized vector of embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-GgqScafZtFJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JFpa6O6EZtFK"
      },
      "outputs": [],
      "source": [
        "def embed(text):\n",
        "    return embed_hub([text])[0].numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlbb0DXIZtFL"
      },
      "source": [
        "Here we check that embeddings are of the same size and type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wSLLwfCDZtFM"
      },
      "outputs": [],
      "source": [
        "assert embed(\n",
        "            \"Some random text\"\n",
        "        ).shape == \\\n",
        "        embed(\n",
        "            \"Folks, here's a story about Minnie the Moocher. \"\n",
        "            \"She was a lowdown hoochie coocher. \"\n",
        "            \"She was the roughest, toughest frail, \"\n",
        "            \"but Minnie had a heart as big as a whale\"\n",
        "        ).shape, \"Shape should match\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aui0cnXJZtFN"
      },
      "source": [
        "NB: here we check DISTANCE, not similarity. This similar texts should produce results close to 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "uA9xwQNJZtFN"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "assert abs(cosine(\n",
        "            embed(\"some text for testing\"), \n",
        "            embed(\"some text for testing\")\n",
        "        )) < 1e-4, \"Embedding should match\"\n",
        "\n",
        "assert abs(cosine(\n",
        "            embed(\"Cats eat mice.\"), \n",
        "            embed(\"Terminator is an autonomous cyborg, typically humanoid, originally conceived as a virtually indestructible soldier, infiltrator, and assassin.\")\n",
        "        )) > 0.2, \"Embeddings should be far\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_0Jbow2ZtFO"
      },
      "source": [
        "## 3. Read the data\n",
        "\n",
        "Now, let's read the facts dataset. Download it from the abovementioned url and read to the list of sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "C78rx_x9ZtFO"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "url = \"https://raw.githubusercontent.com/IUCVLab/information-retrieval/main/datasets/facts.txt\"\n",
        "\n",
        "#TODO read facts into a list of facts. Each fact is a separate element of array\n",
        "facts = requests.get(url).text.split('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWCBybmiZtFQ",
        "outputId": "c18f0229-5c80-4d53-a9d2-7f4b7c8428a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. If you somehow found a way to extract all of the gold from the bubbling core of our lovely little planet, you would be able to cover all of the land in a layer of gold up to your knees.\n",
            "2. McDonalds calls frequent buyers of their food \"heavy users.\"\n",
            "3. The average person spends 6 months of their lifetime waiting on a red light to turn green.\n",
            "4. The largest recorded snowflake was in Keogh, MT during year 1887, and was 15 inches wide.\n",
            "5. You burn more calories sleeping than you do watching television.\n"
          ]
        }
      ],
      "source": [
        "print(*facts[:5], sep='\\n')\n",
        "\n",
        "assert len(facts) == 159\n",
        "assert ('our lovely little planet') in facts[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41gDwIILZtFR"
      },
      "source": [
        "## 4. Transform sentences to vectors\n",
        "\n",
        "Transform the list of facts to `numpy.array` of vectors corresponding to each document (`sent_vecs`), inferring them from the model we just loaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "VzhbDAp8ZtFS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "#TODO infer vectors\n",
        "sent_vecs = np.array([embed(fact) for fact in facts])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "IscByyvkZtFS"
      },
      "outputs": [],
      "source": [
        "assert sent_vecs.shape[0] == len(facts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQfq-lAcZtFX"
      },
      "source": [
        "## 5. Find closest to the query\n",
        "\n",
        "Now find 5 facts which are the closest to the query using cosine measure.\n",
        "\n",
        "### 5.1. Closest search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "8VjKxqb6ZtFX"
      },
      "outputs": [],
      "source": [
        "def find_k_closest(query, dataset, k=10):\n",
        "    return np.argsort(dataset @ query)[-k:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGRe0Y_YZtFY"
      },
      "source": [
        "### 5.1. Use your function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCdzJcO-ZtFY",
        "outputId": "e2233d29-566b-4583-ce1e-d7e3f4c4b01d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for query: good mood\n",
            "\n",
            "\t 44. Honey never spoils.\n",
            "\t 45. About half of all Americans are on a diet on any given day.\n",
            "\t 98. Blue-eyed people tend to have the highest tolerance of alcohol.\n",
            "\t 68. Cherophobia is the fear of fun.\n",
            "\t 57. Gorillas burp when they are happy\n"
          ]
        }
      ],
      "source": [
        "query = \"good mood\"\n",
        "query_vec = embed(query)\n",
        "\n",
        "print(\"Results for query:\", query)\n",
        "print()\n",
        "for k in find_k_closest(query_vec, sent_vecs, 5):\n",
        "    print(\"\\t\", facts[k])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRRcQUGwZtFZ"
      },
      "source": [
        "## 6. Measure DCG@5 for the following query bucket\n",
        "```\n",
        "good mood\n",
        "gorilla\n",
        "woman\n",
        "earth\n",
        "japan\n",
        "people\n",
        "math\n",
        "```\n",
        "\n",
        "Recommend 5 facts to each of the queries. Write your code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qj8ExKB2ZtFZ",
        "outputId": "94128a87-c70b-4565-c6a4-4032dc6d0b04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "good mood\n",
            "\t 57. Gorillas burp when they are happy\n",
            "\t 68. Cherophobia is the fear of fun.\n",
            "\t 98. Blue-eyed people tend to have the highest tolerance of alcohol.\n",
            "\t 45. About half of all Americans are on a diet on any given day.\n",
            "\t 44. Honey never spoils.\n",
            "gorilla\n",
            "\t 55. The word \"gorilla\" is derived from a Greek word meaning, \"A tribe of hairy women.\"\n",
            "\t 57. Gorillas burp when they are happy\n",
            "\t 137. Human birth control pills work on gorillas.\n",
            "\t 106. The male ostrich can roar just like a lion.\n",
            "\t 85. The elephant is the only mammal that can't jump!\n",
            "woman\n",
            "\t 151. Women have twice as many pain receptors on their body than men. But a much higher pain tolerance.\n",
            "\t 16. Men are 6 times more likely to be struck by lightning than women.\n",
            "\t 116. Male dogs lift their legs when they are urinating for a reason. They are trying to leave their mark higher so that it gives off the message that they are tall and intimidating.\n",
            "\t 55. The word \"gorilla\" is derived from a Greek word meaning, \"A tribe of hairy women.\"\n",
            "\t 6. There are more lifeforms living on your skin than there are people on the planet.\n",
            "earth\n",
            "\t 88. Earth is the only planet that is not named after a god.\n",
            "\t 21. Earth has traveled more than 5,000 miles in the past 5 minutes.\n",
            "\t 6. There are more lifeforms living on your skin than there are people on the planet.\n",
            "\t 147. Russia has a larger surface area than Pluto.\n",
            "\t 152. There are more stars in space than there are grains of sand on every beach in the world.\n",
            "japan\n",
            "\t 60. It is considered good luck in Japan when a sumo wrestler makes your baby cry.\n",
            "\t 123. There are 5 temples in Kyoto, Japan that have blood stained ceilings. The ceilings are made from the floorboards of a castle where warriors killed themselves after a long hold-off against an army. To this day, you can still see the outlines and footprints.\n",
            "\t 64. In Japan, crooked teeth are considered cute and attractive.\n",
            "\t 147. Russia has a larger surface area than Pluto.\n",
            "\t 79. A waterfall in Hawaii goes up sometimes instead of down.\n",
            "people\n",
            "\t 6. There are more lifeforms living on your skin than there are people on the planet.\n",
            "\t 34. 95% of people text things they could never say in person.\n",
            "\t 154. The total weight of all those ants, however, is about the same as all the humans.\n",
            "\t 102. More than 50% of the people in the world have never made or received a telephone call.\n",
            "\t 87. If 33 million people held hands, they could make it all the way around the equator.\n",
            "math\n",
            "\t 119. Dogs are capable of understanding up to 250 words and gestures and have demonstrated the ability to do simple mathematical calculations.\n",
            "\t 132. If you started with $0.01 and doubled your money every day, it would take 27 days to become a millionaire.\n",
            "\t 30. If you were to stretch a Slinky out until it's flat, it would measure 87 feet long.\n",
            "\t 94. Of all the words in the English language, the word \"set\" has the most definitions. The word \"run\" comes in close second.\n",
            "\t 27. A mole can dig a tunnel that is 300 feet long in only one night.\n"
          ]
        }
      ],
      "source": [
        "bucket = \"\"\"good mood\n",
        "gorilla\n",
        "woman\n",
        "earth\n",
        "japan\n",
        "people\n",
        "math\"\"\".split('\\n')\n",
        "\n",
        "for term in bucket:\n",
        "    print(term)\n",
        "    for k in find_k_closest(embed(term), sent_vecs, k=5)[::-1]:\n",
        "        print(\"\\t\", facts[k])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPXPt1IyZtFa"
      },
      "source": [
        "## 7. Write your own relevance assessments and compute DCG@5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ic2KGgPUZtFa",
        "outputId": "3a9b0ce3-6ad7-42ee-efcc-71bc13b55807"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DCG@5 = 1.5029\n",
            "IDCG@5 = 2.9485\n",
            "nDCG@5 = 0.5097\n"
          ]
        }
      ],
      "source": [
        "assessments = [\n",
        "    [0, 0, 0, 0, 0], # good mood\n",
        "    [0, 0, 0, 0, 0], # gorilla\n",
        "    [0, 0, 0, 0, 0], # woman\n",
        "    [0, 0, 0, 0, 0], # earth\n",
        "    [0, 0, 0, 0, 0], # japan\n",
        "    [0, 0, 0, 0, 0], # people\n",
        "    [0, 0, 0, 0, 0], # math\n",
        "]\n",
        "\n",
        "dcg = sum([dcg(row) for row in assessments]) / len(assessments)\n",
        "idcg = sum([dcg(row) for row in assessments]) / len(assessments)\n",
        "\n",
        "print(f\"DCG@5 = {dcg5:.4f}\")\n",
        "print(f\"IDCG@5 = {idcg5:.4f}\")\n",
        "print(f\"nDCG@5 = {dcg5 / idcg5:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}