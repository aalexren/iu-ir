{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Homework #1\n",
        "Artem Chernitsa, B20-AI, a.chernitsa@innopolis.university"
      ],
      "metadata": {
        "id": "s2S0U5eRPmdI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56NIEU5bXB_F"
      },
      "source": [
        "# 1. Crawler\n",
        "\n",
        "## 1.0. Related example\n",
        "\n",
        "This code shows `wget`-like tool written in python. Run it from console (`python wget.py`), make it work. Check the code, reuse, and modify for your needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "F0B5c6s1XB_I"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import re\n",
        "import requests\n",
        "\n",
        "\n",
        "# def wget(url, filename):\n",
        "#     # allow redirects - in case file is relocated\n",
        "#     resp = requests.get(url, allow_redirects=True)\n",
        "#     # this can also be 2xx, but for simplicity now we stick to 200\n",
        "#     # you can also check for `resp.ok`\n",
        "#     if resp.status_code != 200:\n",
        "#         print(resp.status_code, resp.reason, 'for', url)\n",
        "#         return\n",
        "    \n",
        "#     # just to be cool and print something\n",
        "#     print(*[f\"{key}: {value}\" for key, value in resp.headers.items()], sep='\\n')\n",
        "#     print()\n",
        "    \n",
        "#     # try to extract filename from url\n",
        "#     if filename is None:\n",
        "#         # start with http*, ends if ? or # appears (or none of)\n",
        "#         m = re.search(\"^http.*/([^/\\?#]*)[\\?#]?\", url)\n",
        "#         filename = m.group(1)\n",
        "#         if not filename:\n",
        "#             raise NameError(f\"Filename neither given, nor found for {url}\")\n",
        "\n",
        "#     # what will you do in case 2 websites store file with the same name?\n",
        "#     if os.path.exists(filename):\n",
        "#         raise OSError(f\"File {filename} already exists\")\n",
        "    \n",
        "#     with open(filename, 'wb') as f:\n",
        "#         f.write(resp.content)\n",
        "#         print(f\"File saved as {filename}\")\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     parser = argparse.ArgumentParser(description='download file.')\n",
        "#     parser.add_argument(\"-O\", type=str, default=None, dest='filename', help=\"output file name. Default -- taken from resource\")\n",
        "#     parser.add_argument(\"url\", type=str, default=None, help=\"Provide URL here\")\n",
        "#     args = parser.parse_args()\n",
        "#     wget(args.url, args.filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSlOaQkXXB_L"
      },
      "source": [
        "### 1.0.1. How to parse a page?\n",
        "\n",
        "If you build a crawler, you might follow one of the approaches:\n",
        "1. search for URLs in the page, assuming this is just a text.\n",
        "2. search for URLs in the places where URLs should appear: `<a href=..`, `<img src=...`, `<iframe src=...` and so on.\n",
        "\n",
        "To follow the first approach you can rely on some good regular expression. [Like this](https://stackoverflow.com/a/3809435).\n",
        "\n",
        "To follow the second approach just read one of these: [short answer](https://stackoverflow.com/questions/1080411/retrieve-links-from-web-page-using-python-and-beautifulsoup) or [exhaustive explanation](https://hackersandslackers.com/scraping-urls-with-beautifulsoup/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmfK7xCoXB_M"
      },
      "source": [
        "## 1.1. [15] Download and persist #\n",
        "Please complete a code for `load()`, `download()` and `persist()` methods of `Document` class. What they do:\n",
        "- for a given URL `download()` method downloads binary data and stores in `self.content`. It returns `True` for success, else `False`.\n",
        "- `persist()` method saves `self.content` somewhere in file system. We do it to avoid multiple downloads (for caching in other words).\n",
        "- `load()` method loads data from hard drive. Returns `True` for success.\n",
        "\n",
        "Tests checks that your code somehow works.\n",
        "\n",
        "**NB Passing the test doesn't mean you correctly completed the task.** These are **criteria, which have to be fullfilled**:\n",
        "1. URL is a unique identifier (as it is a subset of URI). Thus, documents with different URLs should be stored in different files. Typical errors: documents from the same domain are overwritten to the same file, URLs with similar endings are downloaded to the same file, etc.\n",
        "2. The document can be not only a text file, but also a binary. Pay attention that if you download `mp3` file, it still can be played. Hint: don't hurry to convert everything to text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "imK9ApAgXB_P"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from urllib.parse import quote\n",
        "import os\n",
        "\n",
        "class Document:\n",
        "    \n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "        self.file_name = quote(url, safe='')\n",
        "        \n",
        "    def get(self):\n",
        "        if not self.load():\n",
        "            if not self.download():\n",
        "                raise FileNotFoundError(self.url)\n",
        "            else:\n",
        "                self.persist()\n",
        "    \n",
        "    def download(self):\n",
        "        try:\n",
        "            response = requests.get(self.url, timeout=(5, 30))\n",
        "            if response.status_code == 200:\n",
        "                self.content = response.content\n",
        "                return True\n",
        "            else:\n",
        "                return False\n",
        "        except:\n",
        "            return False\n",
        "    \n",
        "    def persist(self):\n",
        "        with open(self.file_name, \"wb\") as f:\n",
        "            f.write(self.content)\n",
        "            \n",
        "    def load(self):\n",
        "        if os.path.exists(self.file_name):\n",
        "            with open(self.file_name, \"rb\") as f:\n",
        "                self.content = f.read()\n",
        "            return True\n",
        "        else:\n",
        "            return False\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# requests.head(some_link)\n",
        "# header = h.headers\n",
        "# content_type = header.get('content-type')"
      ],
      "metadata": {
        "id": "gZmTPr8ztLEh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofTSgqf8XB_Q"
      },
      "source": [
        "### 1.1.1. Tests ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EZthqQkRXB_Q"
      },
      "outputs": [],
      "source": [
        "doc = Document('http://sprotasov.ru/data/iu.txt')\n",
        "\n",
        "doc.get()\n",
        "assert doc.content, \"Document download failed\"\n",
        "assert \"Code snippets, demos and labs for the course\" in str(doc.content), \"Document content error\"\n",
        "\n",
        "doc.get()\n",
        "assert doc.load(), \"Load should return true for saved document\"\n",
        "assert \"Code snippets, demos and labs for the course\" in str(doc.content), \"Document load from disk error\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = Document('https://cdn.drivemusic.me/dl/online/ZhzI3EFVh2ScpkkYhtRf4Q/1675321813/download_music/2014/05/nico-vinz-am-i-wrong.mp3')\n",
        "doc.get()"
      ],
      "metadata": {
        "id": "6-TavmHtYF9C"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYqeLrUSXB_Q"
      },
      "source": [
        "## 1.2. [10] Parse HTML\n",
        "`BeautifulSoap` library is a de facto standard to parse XML and HTML documents in python. Use it to complete `parse()` method that extracts document contents. You should initialize:\n",
        "1. `self.anchors` list of tuples `('text', 'url')` met in a document. Be aware, there exist relative links (e.g. `../content/pic.jpg`). Use `urllib.parse.urljoin()` to fix this issue.\n",
        "2. `self.images` list of images met in a document. Again, links can be relative to current page.\n",
        "3. `self.text` should keep plain text of the document without scripts, tags, comments and so on. You can refer to [this stackoverflow answer](https://stackoverflow.com/a/1983219) for details.\n",
        "\n",
        "**NB All these 3 criteria must be fulfilled to get full point for the task.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MEsg4DJaXB_R"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from bs4.element import Comment\n",
        "import urllib.parse\n",
        "\n",
        "\n",
        "class HtmlDocument(Document):\n",
        "    \n",
        "    def parse(self):\n",
        "        #TODO extract plain text, images and links from the document\n",
        "        self.anchors = [(\"fake link text\", \"http://fake.url/\")]\n",
        "        self.images = [\"http://image.com/fake.jpg\"]\n",
        "        self.text = \"fake text and some other text\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from bs4.element import Comment\n",
        "import urllib.parse\n",
        "\n",
        "\n",
        "class HtmlDocument(Document):\n",
        "    \n",
        "    def parse(self):\n",
        "        soup = BeautifulSoup(self.content, 'html.parser', from_encoding='utf-8')\n",
        "\n",
        "        # extract links\n",
        "        self.anchors = []\n",
        "        for link in soup.find_all('a'):\n",
        "            text = link.text\n",
        "            href = link.get('href')\n",
        "            # fix relative links using urllib.parse.urljoin()\n",
        "            url = urllib.parse.urljoin(self.url, href)\n",
        "            self.anchors.append((text, url))\n",
        "\n",
        "        # extract images\n",
        "        self.images = []\n",
        "        for img in soup.find_all('img'):\n",
        "            src = img.get('src')\n",
        "            # fix relative links using urllib.parse.urljoin()\n",
        "            url = urllib.parse.urljoin(self.url, src)\n",
        "            self.images.append(url)\n",
        "\n",
        "        # extract plain text\n",
        "        def tag_visible(element):\n",
        "            if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
        "                return False\n",
        "            if isinstance(element, Comment):\n",
        "                return False\n",
        "            return True\n",
        "        texts = soup.findAll(text=True)\n",
        "        visible_texts = filter(tag_visible, texts)\n",
        "        self.text = \" \".join(t.strip() for t in visible_texts)"
      ],
      "metadata": {
        "id": "N5KMmmSoawGR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIKfXA5nXB_R"
      },
      "source": [
        "### 1.2.1. Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wO28sVNZXB_R"
      },
      "outputs": [],
      "source": [
        "doc = HtmlDocument(\"http://sprotasov.ru\")\n",
        "doc.get()\n",
        "doc.parse()\n",
        "\n",
        "assert \"just few links\" in doc.text, \"Error parsing text\"\n",
        "assert \"http://sprotasov.ru/images/gb.svg\" in doc.images, \"Error parsing images\"\n",
        "assert any(p[1] == \"https://twitter.com/07C3\" for p in doc.anchors), \"Error parsing links\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(*doc.anchors, sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3t2nB2JQLIx",
        "outputId": "8ed28937-f3a4-4932-f4a7-7782683d732c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('telegram', 'https://t.me/sprotasov')\n",
            "('email', 'mailto:stanislav.protasov@gmail.com')\n",
            "('Curriculum vitae', 'https://docs.google.com/document/d/e/2PACX-1vQqlsxmlbkwp7CypdNg5vcl9zEfE1w6EFppJ2iBbHpZrpOI0AIzFkeu21-Or1_PYlnq1ICyLR1qaNlu/pub')\n",
            "('Google Scholar', 'https://scholar.google.ru/citations?user=pDske8oAAAAJ')\n",
            "('GitHub', 'https://github.com/str-anger')\n",
            "('Track record in Quantum', 'http://sprotasov.ru/q.html')\n",
            "('ResearchGate', 'https://www.researchgate.net/profile/Stanislav-Protasov')\n",
            "('Публикации в eLibrary', 'http://elibrary.ru/author_items.asp?authorid=789317')\n",
            "('Facebook', 'https://www.facebook.com/stanislav.protasov')\n",
            "('LinkedIn', 'https://www.linkedin.com/pub/stanislav-protasov/28/651/b38')\n",
            "('Research with Stas telegram channel', 'https://t.me/iu_aml')\n",
            "('Подкаст \"Происхождение видов\": telegram', 'https://t.me/origin_of_species')\n",
            "('iTunes', 'https://itunes.apple.com/ru/podcast/происхождение-видов/id1282666034')\n",
            "('RSS', 'http://sprotasov.ru/podcast/rss.xml')\n",
            "('Automatic testing system', 'http://code-test.ru/')\n",
            "('source code', 'https://bitbucket.org/str-anger/stick-rope')\n",
            "('Книга \"Давайте объясню: или зачем программисту математика\"', 'http://sprotasov.ru/math_book.html')\n",
            "('Материалы на ПостНауке', 'https://postnauka.ru/themes/protasov')\n",
            "('Twitter', 'https://twitter.com/07C3')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA0wmFA5XB_S"
      },
      "source": [
        "## 1.3. [10] Document analysis ##\n",
        "Complete the code for `HtmlDocumentTextData` class. Implement word and sentence splitting (use any method you can propose). \n",
        "\n",
        "**Criteria to succeed in the task**: \n",
        "1. Your `get_word_stats()` method should return `Counter` object.\n",
        "2. Don't forget to lowercase your words for counting.\n",
        "3. Sentences should be obtained from inside `<body>` tag only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2Ic_-bAZXB_S"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "class HtmlDocumentTextData:\n",
        "    def __init__(self, url):\n",
        "        self.doc = HtmlDocument(url)\n",
        "        self.doc.get()\n",
        "        self.doc.parse()\n",
        "    \n",
        "    def get_sentences(self):\n",
        "        # Get text within <body> tag only\n",
        "        soup = BeautifulSoup(self.doc.content, 'html.parser', from_encoding='utf-8')\n",
        "        body = soup.find(\"body\")\n",
        "        text = body.get_text(\" \", strip=True) if body else \"\"\n",
        "\n",
        "        # split text into sentences using regex\n",
        "        pattern = re.compile(r\"[.!?][\\s]+\")\n",
        "        sentences = pattern.split(text)\n",
        "\n",
        "        return sentences\n",
        "\n",
        "    def get_word_stats(self):\n",
        "        words = []\n",
        "        for sentence in self.get_sentences():\n",
        "            # Flatten list\n",
        "            words += re.findall(r'\\b\\w+\\b', sentence.lower())\n",
        "            # words.extend(sentence.lower().split())\n",
        "\n",
        "        # words = re.findall(r'\\b\\w+\\b', self.text.lower())\n",
        "\n",
        "        return Counter(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzMxZ-I2XB_S"
      },
      "source": [
        "### 1.3.1. Tests ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "uvHDtCbyXB_T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90cc91be-767c-441c-9daf-3299605ae8c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('и', 46), ('в', 23), ('иннополис', 22), ('на', 14), ('с', 14), ('университет', 12), ('университета', 12), ('по', 10), ('ит', 10), ('центр', 10)]\n"
          ]
        }
      ],
      "source": [
        "doc = HtmlDocumentTextData(\"https://innopolis.university/\")\n",
        "\n",
        "print(doc.get_word_stats().most_common(10))\n",
        "assert [x for x in doc.get_word_stats().most_common(10) if x[0] == 'иннополис'], 'иннополис should be among most common'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSCZe0nmXB_X"
      },
      "source": [
        "## 1.4. [15] Crawling ##\n",
        "\n",
        "Method `crawl_generator()` is given starting url (`source`) and max depth of search. It should return a **generator** of `HtmlDocumentTextData` objects (return a document as soon as it is downloaded and parsed). You can benefit from `yield obj_name` python construction. Use `HtmlDocumentTextData.anchors` field to go deeper."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.parse\n",
        "\n",
        "def parse_robots_txt(base_url):\n",
        "    robots_url = urllib.parse.urljoin(base_url, '/robots.txt')\n",
        "    response = requests.get(robots_url)\n",
        "    content = response.text\n",
        "\n",
        "    disallowed_urls = []\n",
        "    for line in content.splitlines():\n",
        "        if line.startswith('Disallow:'):\n",
        "            path = line.split(':', 1)[1].strip()\n",
        "            if path:\n",
        "                disallowed_urls.append(urllib.parse.urljoin(base_url, path))\n",
        "    return set(disallowed_urls)\n",
        "\n",
        "def get_base_url(full_url):\n",
        "    parsed_url = urllib.parse.urlparse(full_url)\n",
        "    base_url = parsed_url.scheme + \"://\" + parsed_url.netloc\n",
        "    return base_url"
      ],
      "metadata": {
        "id": "H38ZDJT_w5_3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "egvDGXy4XB_X"
      },
      "outputs": [],
      "source": [
        "from queue import Queue\n",
        "from hashlib import md5\n",
        "\n",
        "class Crawler:\n",
        "    \n",
        "    def crawl_generator(self, source, depth=1):\n",
        "        visited = set()\n",
        "        queue = Queue()\n",
        "        queue.put((source, 0))\n",
        "\n",
        "        base_url = get_base_url(source)\n",
        "        visited = visited.union(parse_robots_txt(base_url))\n",
        "\n",
        "        while not queue.empty():\n",
        "            url, d = queue.get()\n",
        "            url_hash = md5(url.encode()).hexdigest()\n",
        "            if url_hash in visited:\n",
        "                continue\n",
        "            visited.add(url_hash)\n",
        "            \n",
        "            # print(f'I process this url: {url}')\n",
        "\n",
        "            try:\n",
        "                doc = HtmlDocumentTextData(url)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            if d == depth:\n",
        "                continue\n",
        "            \n",
        "            yield doc\n",
        "\n",
        "            # print(f'd = {d}')\n",
        "\n",
        "            # print(len(doc.doc.anchors))\n",
        "            for _, link in doc.doc.anchors:\n",
        "                queue.put((link, d + 1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJDjbcbaXB_X"
      },
      "source": [
        "### 1.4.1. Tests ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MfxQvFZ1XB_Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e315984-7897-4c82-eb51-6666984b3931"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://innopolis.university/en/\n",
            "339 distinct word(s) so far\n",
            "https://innopolis.university/\n",
            "876 distinct word(s) so far\n",
            "https://apply.innopolis.university/en\n",
            "1496 distinct word(s) so far\n",
            "https://innopolis.university/lk/\n",
            "1505 distinct word(s) so far\n",
            "https://innopolis.university/en/about/\n",
            "1685 distinct word(s) so far\n",
            "https://innopolis.university/en/board/\n",
            "1762 distinct word(s) so far\n",
            "https://innopolis.university/en/team/\n",
            "1763 distinct word(s) so far\n",
            "https://innopolis.university/en/team-structure/\n",
            "1766 distinct word(s) so far\n",
            "https://innopolis.university/en/team-structure/education-academics/\n",
            "1768 distinct word(s) so far\n",
            "https://innopolis.university/en/team-structure/techcenters/\n",
            "1773 distinct word(s) so far\n",
            "https://innopolis.university/en/faculty/\n",
            "2642 distinct word(s) so far\n",
            "https://career.innopolis.university/en/job/\n",
            "3088 distinct word(s) so far\n",
            "https://career.innopolis.university/en/\n",
            "3197 distinct word(s) so far\n",
            "https://innopolis.university/en/campus\n",
            "3310 distinct word(s) so far\n",
            "https://innopolis.university/en/contacts/\n",
            "3317 distinct word(s) so far\n",
            "https://apply.innopolis.university/en/\n",
            "3317 distinct word(s) so far\n",
            "https://apply.innopolis.university/en/bachelor/\n",
            "3374 distinct word(s) so far\n",
            "https://apply.innopolis.university/en/master/\n",
            "3388 distinct word(s) so far\n",
            "https://apply.innopolis.university/en/postgraduate-study/\n",
            "3418 distinct word(s) so far\n",
            "https://apply.innopolis.university/en/stud-life/\n",
            "3479 distinct word(s) so far\n",
            "https://innopolis.university/en/international-relations-office/\n",
            "3705 distinct word(s) so far\n",
            "https://innopolis.university/en/incomingstudents/\n",
            "4067 distinct word(s) so far\n",
            "https://innopolis.university/en/outgoingstudents/\n",
            "4341 distinct word(s) so far\n",
            "https://innopolis.university/en/teachingexcellencecenter/\n",
            "4515 distinct word(s) so far\n",
            "https://innopolis.university/en/writinghubhome/\n",
            "4527 distinct word(s) so far\n",
            "https://alumni.innopolis.university/\n",
            "4668 distinct word(s) so far\n",
            "https://innopolis.university/en/research/\n",
            "4692 distinct word(s) so far\n",
            "https://innopolis.university/en/team-structure/team-faculty/\n",
            "4704 distinct word(s) so far\n",
            "https://innopolis.university/en/team-structure/team-faculty2/\n",
            "4711 distinct word(s) so far\n",
            "https://innopolis.university/en/nir2022/\n",
            "4868 distinct word(s) so far\n",
            "https://innopolis.university/en/proekty/activity/\n",
            "4898 distinct word(s) so far\n",
            "https://innopolis.university/en/startupstudio/\n",
            "4929 distinct word(s) so far\n",
            "https://innopolis.university/en/internationalpartners/\n",
            "4995 distinct word(s) so far\n",
            "https://innopolis.university/en/organizatsiya-i-provedenie-meropriyatiy/\n",
            "5079 distinct word(s) so far\n",
            "https://innopolis.university/en/?special=Y\n",
            "5088 distinct word(s) so far\n",
            "https://innopolis.university/search/\n",
            "5091 distinct word(s) so far\n",
            "https://innopolis.university/en/ido/\n",
            "5116 distinct word(s) so far\n",
            "https://dovuz.innopolis.university/\n",
            "5120 distinct word(s) so far\n",
            "http://www.campuslife.innopolis.ru\n",
            "11493 distinct word(s) so far\n",
            "https://media.innopolis.university/news/webinar-interstudents-eng/\n",
            "11552 distinct word(s) so far\n",
            "https://media.innopolis.university/news/devops-summer-school/\n",
            "11699 distinct word(s) so far\n",
            "https://media.innopolis.university/news/webinar-for-international-candidates-/\n",
            "11706 distinct word(s) so far\n",
            "https://media.innopolis.university/news/registration-innopolis-open-2020/\n",
            "11801 distinct word(s) so far\n",
            "https://media.innopolis.university/news/cyber-resilience-petrenko/\n",
            "11970 distinct word(s) so far\n",
            "https://media.innopolis.university/news/innopolis-university-extends-international-application-deadline-/\n",
            "11971 distinct word(s) so far\n",
            "https://media.innopolis.university/news/self-driven-school/\n",
            "12043 distinct word(s) so far\n",
            "https://innopolis.university/en/form/\n",
            "12101 distinct word(s) so far\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://innopolis.university/public/files/Consent_to_the_processing_of_PD_for_UI.pdf\n",
            "Skipping https://innopolis.university/public/files/Consent_to_the_processing_of_PD_for_UI.pdf\n",
            "https://t.me/universityinnopolis\n",
            "12137 distinct word(s) so far\n",
            "https://vk.com/innopolisu\n",
            "12319 distinct word(s) so far\n",
            "https://www.youtube.com/user/InnopolisU\n",
            "13970 distinct word(s) so far\n",
            "https://apply.innopolis.ru/en/\n",
            "15568 distinct word(s) so far\n",
            "https://panoroo.com/virtual-tours/NvQZM6B2\n",
            "15590 distinct word(s) so far\n",
            "https://media.innopolis.university/en/events/\n",
            "15592 distinct word(s) so far\n",
            "https://minobrnauki.gov.ru/\n",
            "15798 distinct word(s) so far\n",
            "https://career.innopolis.university/konkursnyezayavkiprofessorskoprepodavatelskogosostava/\n",
            "15821 distinct word(s) so far\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
            "WARNING:bs4.dammit:Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n",
            "[('1', 2821), ('the', 2730), ('and', 2525), ('of', 2268), ('0', 1854), ('2', 1759), ('to', 1454), ('in', 1418), ('university', 1244), ('3', 1237), ('4', 1046), ('a', 1043), ('innopolis', 908), ('50', 848), ('for', 821), ('i', 767), ('5', 707), ('fill', 606), ('6', 600), ('7', 568)]\n"
          ]
        }
      ],
      "source": [
        "crawler = Crawler()\n",
        "counter = Counter()\n",
        "\n",
        "for c in crawler.crawl_generator(\"https://innopolis.university/en/\", 2):\n",
        "    print(c.doc.url)\n",
        "    if c.doc.url[-4:] in ('.pdf', '.mp3', '.avi', '.mp4', '.txt'):\n",
        "        print(\"Skipping\", c.doc.url)\n",
        "        continue\n",
        "    counter.update(c.get_word_stats())\n",
        "    print(len(counter), \"distinct word(s) so far\")\n",
        "    \n",
        "print(\"Done\")\n",
        "\n",
        "print(counter.most_common(20))\n",
        "assert [x for x in counter.most_common(20) if x[0] == 'innopolis'], 'innopolis sould be among most common'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# url = r'https://career.innopolis.university/konkursnyezayavkiprofessorskoprepodavatelskogosostava/'\n",
        "# url = 'https://career.innopolis.university/public/files/Согласие на обработку ПДн для УИ.pdf'\n",
        "# doc = HtmlDocument(url)\n",
        "# doc.get()\n",
        "# doc.parse()\n",
        "# doc.anchors"
      ],
      "metadata": {
        "id": "07_wcAlDj65V"
      },
      "execution_count": 15,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}